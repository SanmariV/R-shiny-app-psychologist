---
title: "Machine Learning Models"
author: "Sanmari Vivier"
date: "6/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE)
```

```{r}
library(tidyverse)
library(lubridate)
library(tidyquant)
library(reticulate)
library(plotly)
```


```{r}
# Import dataset--------------------------------
data <- read_csv("data/ICD10_F43.2_patients.csv")
data_tib <- as_tibble(data)

# Data wrangling-------------------------------

# Replace sex variables with more descriptive names
data_tib$Sex <- as.character(data_tib$Sex)
data_tib$Sex[data_tib$Sex == "f"] <- "Female"
data_tib$Sex[data_tib$Sex == "m"] <- "Male"
data_tib$Sex <- as.factor(data_tib$Sex)

data_tib <- data_tib %>% 
            mutate(Birth_year = year(`date-of-birth`)) %>%
            glimpse()

data_tib <- data_tib %>% 
    mutate(age = `therapy-year` - Birth_year) %>%
    glimpse()

# How many males in dataset?
males_tib <- data_tib %>% 
    filter(data_tib$Sex == "m") %>% 
    glimpse()

# How many females in dataset?
females_tib <- data_tib %>% 
    filter(data_tib$Sex == "f") %>% 
    glimpse()

# Visualise the dataset
fig <- ggplot(data = data_tib, aes(x = age, y = `total-sessions`, color = Sex)) + 
    geom_point(alpha = 0.5) + 
    labs(
        title = "Visualisation of data set",
        subtitle = "Therapy sessions according to age and sex",
        x_lab= "Age",
        y_lab="Sessions")+
    theme_tq()

# Add interactivity
ggplotly(fig)

# Remove outliers
# Remove all ages less than 13 and more than 70
# Remove all data where 4 < total-sessions <= 20

data_tib <- data_tib %>% 
            filter(age > 12, age < 71, `total-sessions`<= 20,`total-sessions`>4) %>% 
            glimpse()

# check if outliers were removed
summary(data_tib)
fig_no_outliers <- ggplot(data = data_tib, aes(x = age, y = `total-sessions`, color = Sex)) + 
    geom_point(alpha = 0.5) + 
    labs(
        title = "Visualisation of data set",
        subtitle = "Therapy sessions according to age and sex",
        x_lab= "Age",
        y_lab="Sessions")+
    theme_tq()

# Visualisation with outliers removed
fig_no_outliers
ggplotly(fig_no_outliers)

# Split up data into females and males
males_tib <- data_tib %>% 
    filter(Sex == "Male") %>% 
    glimpse()

females_tib <- data_tib %>% 
    filter(Sex =="Female") %>% 
    glimpse()

# Split data up into data and labels (prepare to apply machine learning model)
female_dataset <- females_tib %>% 
                select(age) %>% 
                glimpse()

female_labels <- females_tib %>% 
                select(`total-sessions`) %>% 
                glimpse()

males_dataset <- males_tib %>% 
                select(age) %>% 
                glimpse()

males_labels <- males_tib %>% 
                select(`total-sessions`) %>% 
                glimpse()
```


```{r}
library(tidyverse)
library(reticulate)
```
```{r}
# set conda environment
use_condaenv("py3.8", required = TRUE)

# check if the correct environment is being used
py_config()
```


```{python}
# is python working?
1+1
```


```{python}
# XGBoost algorithm--------------------------------------------
# import python libraries here
# in conda terminal (with py3.8 env activated, install xgboost): pip install xgboost
import xgboost as xgb
import sklearn
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
```


```{r}
# write tibble files to .csv files
female_dataset_py <- write_csv(female_dataset, 
                               path = "female_dataset_py.csv")
female_labels_py <- write_csv(female_labels,
                              path = "female_labels_py.csv")
males_dataset_py <- write_csv(males_dataset,
                              path = "males_dataset_py.csv")
males_labels_py <- write_csv(males_labels,
                             path = "males_labels_py.csv")
```


```{python}
# Import the created .csv files
# Females
data_female = pd.read_csv(r'C:/Users/Sanmari Vivier/Dropbox/Personal/r-shiny-app/r-markdown-files/female_dataset_py.csv')
data_female_labels = pd.read_csv(r'C:/Users/Sanmari Vivier/Dropbox/Personal/r-shiny-app/r-markdown-files/female_labels_py.csv')
female_dataset_py = pd.DataFrame(data_female)
female_labels_py = pd.DataFrame(data_female_labels)

# Check to see if it is working
print (female_dataset_py.head(), female_labels_py.head())
```


```{python}
# Import the created .csv files----------------------------------
# Males
data_male = pd.read_csv(r'C:/Users/Sanmari Vivier/Dropbox/Personal/r-shiny-app/r-markdown-files/males_dataset_py.csv')
data_male_labels = pd.read_csv(r'C:/Users/Sanmari Vivier/Dropbox/Personal/r-shiny-app/r-markdown-files/males_labels_py.csv')
male_dataset_py = pd.DataFrame(data_male)
male_labels_py = pd.DataFrame(data_male_labels)

# Check to see if it is working
print (male_dataset_py.head(), male_labels_py.head())
```


```{python}
# Split up the data into training and testing sets
# Females

females_data_train, females_data_test = train_test_split(female_dataset_py,
                 test_size=0.33, 
                 random_state=42)
females_labels_train, females_labels_test = train_test_split(female_labels_py,
                 test_size=0.33, 
                 random_state=42)
```
```{python}
# Split up the data into training and testing sets
# Males

males_data_train, males_data_test = train_test_split(male_dataset_py,
                 test_size=0.33, 
                 random_state=42)
males_labels_train, males_labels_test = train_test_split(male_labels_py,
                 test_size=0.33, 
                 random_state=42)

# Check if the data is split                 
print(males_labels_train)
```
# Model 1: XGBoost Classifier algorithm------------------------
```{python}
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Change the data shape of the labels
females_labels_train = np.ravel(females_labels_train)
females_labels_test = np.ravel(females_labels_test)
males_labels_train = np.ravel(males_labels_train)
males_labels_test = np.ravel(males_labels_test)

# Train the model
model = XGBClassifier(max_depth=6,
                    eta=0.01,
                    gamma=4,
                    min_child_weight=6,
                    subsample=0.8,
                    #silent=0,
                    objective='binary:logistic',
                    n_estimators=5,
                    seed=1729
                    )
                                        
model_xgb_f = model
model_xgb_m = model

model_xgb_f.fit(females_data_train, females_labels_train)
model_xgb_m.fit(males_data_train, males_labels_train)

# Test model
pred_xgb_f = model_xgb_f.predict(females_data_test)
pred_xgb_m = model_xgb_m.predict(males_data_test)
xgb_acc_f = accuracy_score(females_labels_test, pred_xgb_f)
xgb_acc_m = accuracy_score(males_labels_test, pred_xgb_m)
print("\n")
print("XGB model accuracy for females: {:.3f} %\n".format(xgb_acc_f * 100))
print("XGB model accuracy for males: {:.3f} %\n".format(xgb_acc_m * 100))
```

# Model 2: XGBoost Regressor algorithm------------------------
```{python}
# Model 2: XGB Regressor
# Here we will be calculating the loss using RMSE (root-mean-square-error)
from sklearn.metrics import mean_squared_error

model2 = xgb.XGBRegressor(objective='reg:squarederror',                                  n_estimators=12,
                          seed=123,
                          learning_rate=0.45
                          )

# Train the model
model_xgbR_f = model2
model_xgbR_m = model2

model_xgbR_f.fit(females_data_train, females_labels_train, eval_metric='rmse')
model_xgbR_m.fit(males_data_train, males_labels_train, eval_metric='rmse')

# Test model
pred_xgbR_f = model_xgbR_f.predict(females_data_test)
pred_xgbR_m = model_xgbR_m.predict(males_data_test)
rmse_f = np.sqrt(mean_squared_error(females_labels_test, pred_xgbR_f))
rmse_m = np.sqrt(mean_squared_error(males_labels_test, pred_xgbR_m))
print("\n")
print("RMSE for females: {:.3f} \n".format(rmse_f))
print("RMSE for males: {:.3f} \n".format(rmse_m))

```

# Model 3: Naive Bayes algorithm------------------------
```{python}
from sklearn.naive_bayes import GaussianNB

# Change the data shapes for the labels using np.ravel()
# Train set
females_labels_train_nb = np.ravel(females_labels_train)
males_labels_train_nb = np.ravel(males_labels_train)

# Test set
females_labels_test_nb = np.ravel(females_labels_test)
males_labels_test_nb = np.ravel(males_labels_test)

model3 = GaussianNB(var_smoothing=0.000000001)

# Train the model
model_nb_f = model3
model_nb_m = model3

model_nb_f.fit(females_data_train, females_labels_train_nb)
model_nb_m.fit(males_data_train, males_labels_train_nb)

# Test model
pred_nb_f = model_nb_f.predict(females_data_test)
pred_nb_m = model_nb_m.predict(males_data_test)
nb_acc_f = accuracy_score(females_labels_test_nb, pred_nb_f)
nb_acc_m = accuracy_score(males_labels_test_nb, pred_nb_m)
print("\n")
print("NB model accuracy for females: {:.3f} %\n".format(nb_acc_f * 100))
print("NB model accuracy for males: {:.3f} %\n".format(nb_acc_m * 100))
```

# Model 4: Support Vector Machine algorithm------------------
```{python}
#from sklearn.svm import SVC
from sklearn.svm import LinearSVC

# Change the data shapes for the labels using np.ravel()
# Train set
females_labels_train_svm = np.ravel(females_labels_train)
males_labels_train_svm = np.ravel(males_labels_train)

# Test set
females_labels_test_svm = np.ravel(females_labels_test)
males_labels_test_svm = np.ravel(males_labels_test)

model4 = LinearSVC(C = 1000, 
                    max_iter=1000000,
                    dual=False
                    )

# Train the model
model_svm_f = model4
model_svm_m = model4

model_svm_f.fit(females_data_train, females_labels_train_svm)
model_svm_m.fit(males_data_train, males_labels_train_svm)

# Test model
pred_svm_f = model_svm_f.predict(females_data_test)
pred_svm_m = model_svm_m.predict(males_data_test)
svm_acc_f = accuracy_score(females_labels_test_svm, pred_svm_f)
svm_acc_m = accuracy_score(males_labels_test_svm, pred_svm_m)
print("\n")
print("SVM model accuracy for females: {:.3f} %\n".format(svm_acc_f * 100))
print("SVM model accuracy for males: {:.3f} %\n".format(svm_acc_m * 100))
```

# Model 5: Logistic Regression algorithm------------------
```{python}
from sklearn.linear_model import LogisticRegression

model5 = LogisticRegression(max_iter=100000,
                            C=1,
                            solver='saga',
                            dual=False
                            )

# Train the model
model_lr_f = model5
model_lr_m = model5

model_lr_f.fit(females_data_train, females_labels_train_nb)
model_lr_m.fit(males_data_train, males_labels_train_nb)

# Test model
pred_lr_f = model_lr_f.predict(females_data_test)
pred_lr_m = model_lr_m.predict(males_data_test)
lr_acc_f = accuracy_score(females_labels_test_nb, pred_lr_f)
lr_acc_m = accuracy_score(males_labels_test_nb, pred_lr_m)
print("\n")
print("LR model accuracy for females: {:.3f} %\n".format(lr_acc_f * 100))
print("LR model accuracy for males: {:.3f} %\n".format(lr_acc_m * 100))
```

